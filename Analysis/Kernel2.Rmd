---
title: "Traditional approaches with some new tools"
output:
  html_document:
    number_sections: TRUE
    toc: TRUE
    fig_height: 4
    fig_width: 7
    code_folding: show
    highlight: tango
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE)

```

# Introduction

This is my first attempt at a Kaggle kernel, Kaggle seems like a great site so wanted to make a foray.  
Coming from a social science background my approach may seem a little 'old school' but shows how I would approach the modelling process, mixed in with some newer machine learning (aka predictive analytics) approaches.  
Feedback welcome, thanks for taking the time to read. 

```{r, message = FALSE}
# Load packages
library(ggplot2)                                          # visualization
library(dplyr)                                            # data manipulation
library(Amelia)                                           # for visualising missing data
library(gridExtra)                                        # for nicely formatted tables
library(vcd)                                              # odds Ratio and Woolf tests
library(corrplot)                                         # correlograms
library(data.table)                                       # for sparse matrix data table setup
library(Matrix)                                           # for one hot encoding with sparsematrix
library(xgboost)                                          # for xgboost algorithm
library(caret)                                            # for model evaulation and tuning
library(ROCR)                                             # for model evaulation with ROC Curves
library(BaylorEdPsych)                                    # for Little's MCAR test
library(mvnmle)                                           # also needed for Little's MCAR test
```

First lets combine the test and train datasets then get a feel for what data there is to work with.

```{r, message=FALSE, warning=FALSE}
train <- read.csv('../Data/1.Raw/train.csv', stringsAsFactors = F, na.strings=c(""," ","NA"))
test  <- read.csv('../Data/1.Raw/test.csv', stringsAsFactors = F, na.strings=c(""," ","NA"))

full  <- bind_rows(train, test)                             # bind training & test data

summary(full)                                               # check data
```
Here we can see that the summary makes sense for some variables - Age is a continous variable ranging from an infant (0.17 years) up to a possible grandfather (80 years), with 263 missing values (NAs). For other variables, such as the passenger's sex, whether they survived and their port of embarkation it would make more sense for these variables - currently a mix of numeric and charecter variables - to be grouped.  Classifying these variables correctly as factors will mean that any functions, plots or models will behave themselves correctly and produce the expected results.

```{r}
# Define the survived variable
full$Survived <- factor(full$Survived, levels=c(1,0))
levels(full$Survived) <- c("Survived", "Died")
str(full)                                              # re-check data
```


We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. Our first step is to get a feel for that data using EDA.

# Exploratory Data Analysis aka EDA
## What's in the data?

First, it is useful to know what is missing, we have already seen that for age there are 263 missing values but what about the other variables?  The Amelia package in R can not only help visualise what is in the data, but can also help fill in some of the blanks, more on that later.  The missmap function will create a 'map' of variable as columns and observations as rows, ordering the variables by the number of missing values.

```{r}
head(full)
missmap(full, col = c("grey","navy"))                     # first colour is missing, second is observed
```

As the test and train datasets have been combined together, the first variable with the highest degree of 'missingness' is Survived, since this relates to the test data.  The variable with the second highest degree of missing values is age, with quite a lot of missing values across both the test and train datasets. We can also explicitly count the number of NAs in a particular variable.

```{r}
NA_Count <- sapply(full, function(y) sum(is.na(y)))
NA_Count <- data.frame(NA_Count)                          # convert to a data frame
grid.table(NA_Count)                                      # produce the output table
```

As expected, Age does have the highest number of missing values (263), with only Fare having one other missing value.  There are also a very large number of missing cabin numbers.

## Numeric Variable exploration

Next we can begin to look at the variables in more detail.  First, let's explore age.

```{r, message=FALSE, warning=FALSE}
#hist(full$Age) - basic histogram

ggplot (data=full[1:891,], aes(x=Age)) +
  geom_histogram(aes(y=..density..)) +
  geom_density(alpha=.2, colour= "red")  +
  stat_function(fun = dnorm, colour = "navy", args = list(mean = 40, sd = 10)) + # add normal curve
  labs(title="Histogram for Age - with Normal Distribution Curve")

#Ypou could also do a facet plot by asssigning the first hist (above) as Age_hist <- ggplot... 
#then doing Age_hist + facet_grid(. ~ Survived) after, for two seperate histograms
```

It looks like we have few over the age of 60, but let's group the age ranges into groups or bins.  The other aspect to consider is that this is not normally distributed, we have a long tail to the right - it is right skewed. We can see the density curve of passengers (red) is different from the more normally distributed density curve (blue).  For some models this may cause a problem, so it may be neccessary to transform this, porbably using a logarithmic transformation.   

```{r}
full$Agebin <- cut(full$Age, seq(0,90,10), right=FALSE, 
                   labels=c("Under 10", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-79", "80-89")) 
                    # cuts age from 0 to 90 in groups/age bands of 10.  Could be made inclusive i.e. up to ten by using right=TRUE
Agebin_Freq <- cbind( Freq=table(full$Agebin), Cumul=cumsum(table(full$Agebin)), Proportion=prop.table(table(full$Agebin)))
Agebin_Freq <- transform(Agebin_Freq) # convert to a data frame, could also be achieved by Agebin_Freq <- data.frame(Agebin_Freq)
Agebin_Freq$Proportion <- paste(round(Agebin_Freq$Proportion * 100, digits=1),"%",sep="") # Decimal to a percent and round to 1 d.p.
grid.table(Agebin_Freq)                                   
```

As suspected, the actual number over the age of 60 is quite small, so lets combined the over 60s in to a single group then see what the distribution is like of surivival by age group by using some stacked bar charts.

```{r}
# Recode into over 60s factor, the second line moves the over 60s to the correct place at the end of the factors rather than the
# default which would return the 60 Plus BEFORE the Under 10s
full$Agebincmb <- recode_factor(full$Agebin, `60-69` = "60 Plus", `70-79` = "60 Plus", `80-89` = "60 Plus") 
full$Agebincmb <- ordered(full$Agebincmb, levels = c("Under 10", "10-19", "20-29", "30-39", "40-49", "50-59","60 Plus"))

#Same process for generating the table as before, but with the updated combined factor levels
Agebin_Freq <- cbind( Freq=table(full$Agebincmb), Cumul=cumsum(table(full$Agebincmb)), Proportion=prop.table(table(full$Agebincmb)))
Agebin_Freq <- transform(Agebin_Freq) 
Agebin_Freq$Proportion <- paste(round(Agebin_Freq$Proportion * 100, digits=1),"%",sep="") 
grid.table(Agebin_Freq) 

#Next we are going to create two cross tabs for the creation of bar charts, one including NA values
agebin_x_survived <-table(full[1:891,]$Agebincmb, full[1:891,]$Survived) # create cross tab for plot
agebin_x_survived <- data.frame(agebin_x_survived) 
names(agebin_x_survived)[names(agebin_x_survived)=="Var1"] <- "Age"
names(agebin_x_survived)[names(agebin_x_survived)=="Var2"] <- "Survived"

agebin_x_survived_NA <- table(full[1:891,]$Agebincmb, full[1:891,]$Survived, useNA = "ifany")
agebin_x_survived_NA <- data.frame(agebin_x_survived_NA) 
names(agebin_x_survived_NA)[names(agebin_x_survived_NA)=="Var1"] <- "Age"
names(agebin_x_survived_NA)[names(agebin_x_survived_NA)=="Var2"] <- "Survived"

#Next we create the two plots based on the two tables just created
p1 <- ggplot(data = agebin_x_survived, aes(x = Age, y = Freq, fill = Survived)) + 
  geom_bar(stat="identity") +
  labs(title="Survival by Age Group") 
  #ADD COUNTS TO CHART or possibly %

p2 <- ggplot(data = agebin_x_survived_NA, aes(x = Age, y = Freq, fill = Survived)) + 
  geom_bar(stat="identity") +
  labs(title="Survival by Age Group - including NAs") 
  #ADD COUNTS TO CHART or possibly %

#we now arrange the two bar charts in a grid with two columns
grid.arrange(p1, p2, ncol=2)
```

The second chart helps to highlight some of the problems with missing (NA) values.  Without the NA values included, there is a large portion of our passengers missing from the analysis.  In addition, it appears that the proportion of those with missing age values who have surivived, is much lower than for some of the other age groups, so we perhaps need to look at this group in more detail or risk missing something important in the underlying structure of survivorship. 

Before proceeding, it is worth looking at the other numeric variable, fare.  We have seen that age is not normally distributed.  What about fare?  

```{r}
ggplot(data=full, aes(Fare)) + 
  geom_histogram() +
  labs(title="Fare Histogram")
```

There is a definite right skew in the Fares, similar to age but much more skewed.  If we remove this skew by log transforming Fare, we can then observe the resulting scatterplot of these two variables, to see if there is a relationship.  We might expect that as the person's age increases, so might the fare paid.  We can calculate the strength and direction of this relationship using Spearman's Rho, which is more suitable for this data than Pearson's measure as we have some outliers for fare, even after transformation, which may determintetally influence the calculation of the Pearson measure.

```{r}
# Log Transform age and produce the scatterplot
full$logFare <- log(full$Fare)

ggplot(full, aes(x=Age, y=logFare)) +
  geom_point() +
  geom_smooth(method=lm) +
  labs(title="Age by Log Transformed Fare")

# Calculate correlation
cor.test(full$Age, full$logFare, method = "spearman")
```

The p-value suggests there is a statistically significant relationship (correlation) between these two variables, however given the rho measure is quite low - 0.19 on a scale of +1 to -1 - this suggest the relationship is weak.


## Dealing with missing values

One option for handling the missing values would be to use an average passenger age across the known values and apply this average to the missing values.  However a more nuanced approach would be to explore the relationships between survival and the other variables at our disposal in the data set, going on to make a more informed estimation of the age value using imputatation.  Earlier we created the missing map using the Amelia package, now we will use the same package to help us impute those missing values for age.  We can think of our options for dealing with missing values as a hierarchy, with increasing levels of complexity and computation:

1. Listwise Deletion (LD) - If a person or case is missing one variable, we can simply delete their case in the analysis.  This can be a safe option and for some models this is the default.  However, we potentially can loose a lot of our data and the ability to draw inferences can be limited.
2. Mean or Median imputation - we take the mean or median value of all cases and use this as a substitute for the missing value.  Whilst this allows us to use cases where some values are missing and therefore can get more value out of our data, there is a risk that those with missing values are in some ways different from the other cases.  If this is the case, then calculating the their value using the average may bias our models.
3a. Multiple imputation - we create various 'versions' of reality, whereby the missing data is guessed according to some process (an algorithm).  Common approaches use a maximum likelihood estimation (MLE - no priors) or maximum a posteriori probability (MAP - with priors), both of which attempt to maximise the probability that a particular value has been observed - the missing value - given our data.  Bootstrap samples of the original data are often taken, meaning the missing values can take more than one possible value, so multiple versions of reality with those other possible values are created, so in effect we end up with many different plausible, complete, datasets.    Usually this approach is parametric, meaning we assume that our data has some underlying structure or distribution, like a normal bell curve or some other distribution form.  This approach allows us to use more of our data, however we have more data handling and computation as a result.  Assumes multivariate normal or log normal plus missing at random (or missing completley at random )



3b.  Non-parametric techniques - we may employ a random forest or k-nearest neighbours (kNN).  It can capture similarity (locality) within the data, but fails to generalise to new cases well.  This can cause problems if our data is generated by a missing at random process.


There are three types of missing data:

* Missing completely at random (MCAR) - there is no difference between the charecteristics of those missing and those observed.  For instance this might happen due to data input errors, made accidentally    
* Missing at Random (MAR) - there is a difference between those missing and those observed, but this can be explained by other variables in the data. For instance if we had missing fare data this might be because those males might be less likely to tell you the fare paid, if collecting the data using a survey.  There is no relationship between the propensity of missing values and the missing data, males may just be less likely to provide this information
* Missing Not at Random (MNAR) - there is a difference between those missing and those observed, AND this CANNOT be explained by other variables in the data.  For instance if we had missing income data and this was directly related to the income. There is a relationship between the value of the missing data and why it is missing, the propensity for the data to be missing is directly related to the value of that variable. 

If our data has both MCAR and MAR missing data we can use missing data techniques.  If the data is MNAR than there a consistent relationship with the missing data which the missing data processes, such as imputation, cannot account for. This can be a problem. Before we begin, we can test whether the variables are MCAR using Little's MCAR test:

* **Null Hypothesis $H_{0}$**: The missing data is Missing Completely At Random (MCAR)
* **Alternate Hypothesis $H_{A}$**: The missing data is NOT Missing Completely At Random (MCAR)

```{r, message=FALSE}
# Select those variables which are litkely to be used by the modelling, for instance we can exlcude things like name and cabin
t1 <- full[c(6:8,10,11,12:13)]

# Run Little's test for MCAR and save it as a new object - some of the output isn't neccessary
t1MCAR <- LittleMCAR(t1)

# Read out just the variables of interest
t1MCAR$amount.missing
t1MCAR$chi.square
t1MCAR$df
t1MCAR$p.value
```

In this instance we can see the p-value is high, so we reject the null hypothesis and accept the alternative hypothesis.  

- Mention cabin

So let's run Little's test without cabin.

```{r, message=FALSE}
# Select those variables which are litkely to be used by the modelling, for instance we can exlcude things like name and cabin
t1 <- full[c(6:8,10,12:13)]

# Run Little's test for MCAR and save it as a new object - some of the output isn't neccessary
t1MCAR <- LittleMCAR(t1)

# Read out just the variable of interest
t1MCAR$p.value
```

So we have a slightly better p-value than including cabin.  However the p-value is still very small indicating there is a very low probability the relationship is due to chance, so we should still reject the null hypothesis that the data is MCAR.  The question then becomes can we ignore the missing data relationship (MAR) or that the missing data is a non-ignorable problem (MNAR).  There isn't a statistical test for MAR, however we can observe the relationship between our variables to see if there is any substative relationship which may suggest it is ignorable or explained.

- Relationships

```{r}
#Change variables into factors not integers
full$Pclass <- as.factor(full$Pclass)
full$PSex <- as.factor(full$Sex)
full$SibSp <- as.factor(full$SibSp)
full$Parch <- as.factor(full$Parch)
full$Cabin <- as.factor(full$Cabin)
full$Embarked <- as.factor(full$Embarked)
full$Survived <- as.factor(full$Survived)


```



```{r}
titanic.amelia <- amelia(subset(full,
                                select=c('Survived','Pclass','Sex','Age')),
                                m=10, noms=c('Survived','Pclass','Sex'),
                                emburn=c(500,500),
                                p2s = F)
```


We also saw a single missing value for fare.

### Cabin 

Image / deck plan
Had hoped to improve by looking through old records, but doesn't seem possible currently.  Looks like it is not just me [with this problem](https://www.kaggle.com/c/titanic/discussion/4693).
Look at correlation between cabin and Pclass - numerically and location (deck plan) wise.



## Two way survival variable exploration 

First, let's see what the age distribution of survivors looks like, by sex and age.

```{r}
ggplot(aes(y = Age, x = Sex, fill = Survived), data = full[1:891,]) + geom_boxplot() +
labs(title="Survival by Sex and Age") # we exclude the NA values
```

Interestingly the average age of males who died is older than those who survived, whereas the situation is reversed for females, with the average age of females who died being younger than those who survived.  Looking at this plot alone, it is a little hard to see if there is a statistically significant relationship.  If we structure the data as a table and run a chi squared test we may get a better idea where there is a true relationship between sex and survival.

```{r}
sex_x_survived <-table(full[1:891,]$Sex, full[1:891,]$Survived)
grid.table(sex_x_survived)
chisq.test(sex_x_survived)
```

But what about surival by sex AND age?  







## Multiple (N) way survival variable exploration 

We have already seen that there does appear to be a statistically significant relationship between the passengers gender (Sex) and their suvival.  If we have continous variables we can use a correlogram, which will help to visualise data and relationships within a correlation table.  Let us see such an example using just the quantative variables.

```{r}
full_reduce <- as.data.frame(full[1:891, c(3,6,7,8,10) ], drop=false)
Var_Corr <- cor(full_reduce)
corrplot(Var_Corr, method="circle")
```

Here we can see that the missing values in age are still causing problems. Clearly this needs to be dealt with before we do any detailed modelling. However,it does suggest a negative relationship with passenger class and the fare that they paid - lower class passengers paid a lower price.  Relationships amungst the other variables appear a little scant, although there does seem to be a positive relationship between the number parents aboard (Parch) and the number of siblings/spouses aboard (SibSp) which we can expect to be the case.  

Using a corrgram should be done with caution at this point, as we ideally should transform all the data so they are on a similar scale. Some of the variables - such as passegner class - are cateogrical variables masquerading as continous variables.  We will address this later.

So we previoulsy found a statistically significant relationship between survival and sex or gender.  We also saw that there appeared to be a relationship by survival and age.  Now we can combine these two variables together and test their relationship against survival, as we are dealing with a categorical variable as the output (surival) we can use a contingency table.  

```{r}
CTab_AgeSex <- xtabs(~full[1:891,]$Sex + full[1:891,]$Survived + full[1:891,]$Agebincmb, data=full)
ftable(CTab_AgeSex)
mantelhaen.test(CTab_AgeSex)
```

First we are using the Cochran–Mantel–Haenszel (CMH) test sees whether we can reject the null hypotesis, which is there is no relationship between sex and survival, after controlling for the different groups - age in our example.  Under the test, each group is treated as different - different location, time or strudy group.  In our example this may be seen as streteching the (statistical) facts a little, although it is not impossible to think of the woman and children getting off the boat at a different time. However, the CMH test is more about testing data that has been collected in more fundamentally different ways, such as testing the sex and surival rates across different boats over different years, where the collection is fundamentally more independent across the different groups. 

With these caveats, the null hypothesis we are testing is that the proportion of people surviving is the same for males and females, after controlling for the the group. In our test, the CMH is statistically significant, indicating that there are more women than men suriving, across the different age groups.

But is this the whole story?  

```{r}
oddsratio(CTab_AgeSex, log=TRUE) 
woolf_test(CTab_AgeSex)
```

We can use GLM with a binomial or logit link aka logistic model.  It is useful to remind ourselves what levels the data are in for the surival outcome variable, since a logistic model takes 'success' as having the second level.  To do so, we use the contrasts function

```{r}
contrasts (full$Survived)
```

So in our case surival is use as the reference level so any stistically significant differences will apply to those who have died.

```{r}
GLM1 <- glm(formula = Survived ~ Sex + Age, family = binomial, data = full[1:891,])
summary (GLM1)
```

```{r}
GLM2 <- glm(formula = Survived ~ Sex * Age, family = binomial, data = full[1:891,])
summary (GLM2)
```




# Lessons Learnt

* Remember to visually inspect the data table - the missing map initially suggested that cabin was not missing at all, until inspecting the data table when it became clear that many values were missing.  This was a real Homer Simpson doh! moment since data often comes with spaces, blank data filled as text and so on, which is usually one of the first things I'd check, yet I hadn't at first.