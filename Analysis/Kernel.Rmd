---
title: 'Exploring the Titanic Dataset :: Traditional approaches with new tools'
author: "James Solomon-Rounce"
date: ' 25th August 2016'
output:
  html_document:
    fig_height: 6
    fig_width: 8
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
---

# Introduction

This is my first attempt at a Kaggle kernel, Kaggle seems like a great site so wanted to make a foray.  Coming from a social science background my approach may seem a little 'old school' but shows how I would approach the modelling process, mixed in with some newer machine learning (aka predictive analytics) approaches.  Feedback welcome, thanks for taking the time to read.  

There are N parts to my script as follows:

* Exploratory Data Analysis aka EDA
* Feature engineering for prediction
* Where is my data or what to do with missing values
* Stick or caret - Predictions using multiple models 

## Load and check data

```{r, message = FALSE}
# Load packages
library('ggplot2')                                          # visualization
library('dplyr')                                            # data manipulation
library('Amelia')                                           # missing values
library('gridExtra')                                        # for nicely formatted tables
library('vcd')                                              # Odds Ratio and Woolf tests
library('corrplot')                                         # Correlograms
library(mice)                                               # for handing missing data
library(data.table)                                         # for sparse matrix data table setup
library(Matrix)                                             # for one hot encoding with sparsematrix
library(xgboost)                                            # for xgboost algorithm
```

First lets combine the test and train datasets then get a feel for what data there is to work with.

```{r, message=FALSE, warning=FALSE}
train <- read.csv('../Data/1.Raw/train.csv', stringsAsFactors = F)
test  <- read.csv('../Data/1.Raw/test.csv', stringsAsFactors = F)

full  <- bind_rows(train, test)                             # bind training & test data

summary(full)                                               # check data
```

Here we can see that the summary makes sense for some variables - Age is a continous variable ranging from an infant (0.17 years) up to a possible grandfather (80 years), with 263 missing values (NAs). For other variables, such as the passenger's sex, whether they survived and their port of embarkation it would make more sense for these variables - currently a mix of numeric and charecter variables - to be groups of individuals, which in R are called factors aka categorical variables.  Classifying these variables correctly as factors will mean that any functions, plots or models will behave themselves correctly and produce the expected results.

```{r}
full$Survived <- factor(full$Survived, levels=c(1,0))
levels(full$Survived) <- c("Survived", "Died")
summary(full)                                              # re-check data
```


We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. Our first step is to get a feel for that data using EDA.

# Exploratory Data Analysis aka EDA
## What's in the data?

First, it is useful to know what is missing, we have already seen that for age there are 263 missing values but what about the other variables?  The Amelia package in R can not only help visualise what is in the data, but can also help fill in some of the blanks, more on that later.  The missmap function will create a 'map' of variable as columns and observations as rows, ordering the variables by the number of missing values.

```{r}
head(full)
missmap(full, col = c("grey","navy"))                     # first colour is missing, second is observed
```

As the test and train datasets have been combined together, the first variable with the highest degree of 'missingness' is Survived, since this relates to the test data.  The variable with the second highest degree of missing values is age, with quite a lot of missing values across both the test and train datasets. We can also explicitly count the number of NAs in a particular variable.

```{r}
NA_Count <- sapply(full, function(y) sum(is.na(y)))
NA_Count <- data.frame(NA_Count)                          # convert to a data frame
grid.table(NA_Count)                                      # produce the output table
```

As expected, Age does have the highest number of missing values (263), with only Fare having one other missing value.

## One way survival variable exploration 

Next we can begin to look at the variables in more detail.  First, let's explore age.

```{r}
#hist(full$Age) - basic histogram

ggplot (data=full[1:891,], aes(x=Age)) +
  geom_histogram(aes(y=..density..)) +
  geom_density(alpha=.2, colour= "red")  +
  stat_function(fun = dnorm, colour = "navy", args = list(mean = 40, sd = 10)) + # add normal curve
  labs(title="Histogram for Age - with Normal Distribution Curve")

#Ypou could also do a facet plot by asssigning the first hist (above) as Age_hist <- ggplot... 
#then doing Age_hist + facet_grid(. ~ Survived) after, for two seperate histograms
```

It looks like we have few over the age of 60, but let's group the age ranges into groups or bins.  The other aspect to consider is that this is not normally distributed, we have a long tail to the right - it is right skewed. We can see the density curve of passengers (red) is different from the more normally distributed density curve (blue).  For some models this may cause a problem, so it may be neccessary to transform this, porbably using a logarithmic transformation.   

```{r}
full$Agebin <- cut(full$Age, seq(0,90,10), right=FALSE, 
                   labels=c("Under 10", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-79", "80-89")) 
                    # cuts age from 0 to 90 in groups/age bands of 10.  Could be made inclusive i.e. up to ten by using right=TRUE
Agebin_Freq <- cbind( Freq=table(full$Agebin), Cumul=cumsum(table(full$Agebin)), Proportion=prop.table(table(full$Agebin)))
Agebin_Freq <- transform(Agebin_Freq) # convert to a data frame, could also be achieved by Agebin_Freq <- data.frame(Agebin_Freq)
Agebin_Freq$Proportion <- paste(round(Agebin_Freq$Proportion * 100, digits=1),"%",sep="") # Decimal to a percent and round to 1 d.p.
grid.table(Agebin_Freq)                                   
```

As suspected, the actual number over the age of 60 is quite small, so lets combined the over 60s in to a single group then see what the distribution is like of surivival by age group by using some stacked bar charts.

```{r}
# Recode into over 60s factor, the second line moves the over 60s to the correct place at the end of the factors rather than the
# default which would return the 60 Plus BEFORE the Under 10s
full$Agebincmb <- recode_factor(full$Agebin, `60-69` = "60 Plus", `70-79` = "60 Plus", `80-89` = "60 Plus") 
full$Agebincmb <- ordered(full$Agebincmb, levels = c("Under 10", "10-19", "20-29", "30-39", "40-49", "50-59","60 Plus"))

#Same process for generating the table as before, but with the updated combined factor levels
Agebin_Freq <- cbind( Freq=table(full$Agebincmb), Cumul=cumsum(table(full$Agebincmb)), Proportion=prop.table(table(full$Agebincmb)))
Agebin_Freq <- transform(Agebin_Freq) 
Agebin_Freq$Proportion <- paste(round(Agebin_Freq$Proportion * 100, digits=1),"%",sep="") 
grid.table(Agebin_Freq) 

#Next we are going to create two cross tabs for the creation of bar charts, one including NA values
agebin_x_survived <-table(full[1:891,]$Agebincmb, full[1:891,]$Survived) # create cross tab for plot
agebin_x_survived <- data.frame(agebin_x_survived) 
names(agebin_x_survived)[names(agebin_x_survived)=="Var1"] <- "Age"
names(agebin_x_survived)[names(agebin_x_survived)=="Var2"] <- "Survived"

agebin_x_survived_NA <- table(full[1:891,]$Agebincmb, full[1:891,]$Survived, useNA = "ifany")
agebin_x_survived_NA <- data.frame(agebin_x_survived_NA) 
names(agebin_x_survived_NA)[names(agebin_x_survived_NA)=="Var1"] <- "Age"
names(agebin_x_survived_NA)[names(agebin_x_survived_NA)=="Var2"] <- "Survived"

#Next we create the two plots based on the two tables just created
p1 <- ggplot(data = agebin_x_survived, aes(x = Age, y = Freq, fill = Survived)) + geom_bar(stat="identity") +
  labs(title="Survival by Age Group") 
  #ADD COUNTS TO CHART or possibly %

p2 <- ggplot(data = agebin_x_survived_NA, aes(x = Age, y = Freq, fill = Survived)) + geom_bar(stat="identity") +
  labs(title="Survival by Age Group - including NAs") 
  #ADD COUNTS TO CHART or possibly %

#we now arrange the two bar charts in a grid with two columns
grid.arrange(p1, p2, ncol=2)
```

The second chart helps to highlight some of the problems with missing (NA) values.  Without the NA values included, there is a large portion of our passengers missing from the analysis.  In addition, it appears that the proportion of those with missing age values who have surivived, is much lower than for some of the other age groups, so we perhaps need to look at this group in more detail or risk missing something important in the underlying structure of survivorship.

One option for handling the missing values would be to use an average passenger age across the known values and apply this average to the missing values.  However a more nuaned approach would be to explore the relationships between survival and the other variables at our disposal in the data set, going on to make a more informed estimation of the age value using imputatation.

## Two way survival variable exploration 

First, let's see what the age distribution of survivors looks like, by sex and age.

```{r}
ggplot(aes(y = Age, x = Sex, fill = Survived), data = full[1:891,]) + geom_boxplot() +
labs(title="Survival by Sex and Age") # we exclude the NA values
```

Interestingly the average age of males who died is older than those who survived, whereas the situation is reversed for females, with the average age of females who died being younger than those who survived.  Looking at this plot alone, it is a little hard to see if there is a statistically significant relationship.  If we structure the data as a table and run a chi squared test we may get a better idea where there is a true relationship between sex and survival.

```{r}
sex_x_survived <-table(full[1:891,]$Sex, full[1:891,]$Survived)
grid.table(sex_x_survived)
chisq.test(sex_x_survived)
```

But what about surival by sex AND age?  







## Multiple (N) way survival variable exploration 

We have already seen that there does appear to be a statistically significant relationship between the passengers gender (Sex) and their suvival.  If we have continous variables we can use a correlogram, which will help to visualise data and relationships within a correlation table.  Let us see such an example using just the quantative variables.

```{r}
full_reduce <- as.data.frame(full[1:891, c(3,6,7,8,10) ], drop=false)
Var_Corr <- cor(full_reduce)
corrplot(Var_Corr, method="circle")
```

Here we can see that the missing values in age are still causing problems. Clearly this needs to be dealt with before we do any detailed modelling. However,it does suggest a negative relationship with passenger class and the fare that they paid - lower class passengers paid a lower price.  Relationships amungst the other variables appear a little scant, although there does seem to be a positive relationship between the number parents aboard (Parch) and the number of siblings/spouses aboard (SibSp) which we can expect to be the case.  

Using a corrgram should be done with caution at this point, as we ideally should transform all the data so they are on a similar scale. Some of the variables - such as passegner class - are cateogrical variables masquerading as continous variables.  We will address this later.

So we previoulsy found a statistically significant relationship between survival and sex or gender.  We also saw that there appeared to be a relationship by survival and age.  Now we can combine these two variables together and test their relationship against survival, as we are dealing with a categorical variable as the output (surival) we can use a contingency table.  

```{r}
CTab_AgeSex <- xtabs(~full[1:891,]$Sex + full[1:891,]$Survived + full[1:891,]$Agebincmb, data=full)
ftable(CTab_AgeSex)
mantelhaen.test(CTab_AgeSex)
```

First we are using the Cochran–Mantel–Haenszel (CMH) test sees whether we can reject the null hypotesis, which is there is no relationship between sex and survival, after controlling for the different groups - age in our example.  Under the test, each group is treated as different - different location, time or strudy group.  In our example this may be seen as streteching the (statistical) facts a little, although it is not impossible to think of the woman and children getting off the boat at a different time. However, the CMH test is more about testing data that has been collected in more fundamentally different ways, such as testing the sex and surival rates across different boats over different years, where the collection is fundamentally more independent across the different groups. 

With these caveats, the null hypothesis we are testing is that the proportion of people surviving is the same for males and females, after controlling for the the group. In our test, the CMH is statistically significant, indicating that there are more women than men suriving, across the different age groups.

But is this the whole story?  

```{r}
oddsratio(CTab_AgeSex, log=TRUE) 
woolf_test(CTab_AgeSex)
```

We can use GLM with a binomial or logit link aka logistic model.  It is useful to remind ourselves what levels the data are in for the surival outcome variable, since a logistic model takes 'success' as having the second level.  To do so, we use the contrasts function

```{r}
contrasts (full$Survived)
```

So in our case surival is use as the reference level so any stistically significant differences will apply to those who have died.

```{r}
GLM1 <- glm(formula = Survived ~ Sex + Age, family = binomial, data = full[1:891,])
summary (GLM1)
```

```{r}
GLM2 <- glm(formula = Survived ~ Sex * Age, family = binomial, data = full[1:891,])
summary (GLM2)
```

## Dealing with missing values

First, let's create a title variable to help with modelling

```{r}
# Grab title from passenger names
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)

# Show title counts by sex
table(full$Sex, full$Title)
```

```{r}
# Titles with very low cell counts to be combined to "rare" level
rare <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')

# Also reassign mlle, ms, and mme accordingly
full$Title[full$Title == 'Mlle']        <- 'Miss' 
full$Title[full$Title == 'Ms']          <- 'Miss'
full$Title[full$Title == 'Mme']         <- 'Mrs' 
full$Title[full$Title %in% rare]        <- 'Rare'

# Show title counts by sex again
table(full$Sex, full$Title)
```

```{r}
# Finally, grab surname from passenger name
full$Surname <- sapply(full$Name,  
                      function(x) strsplit(x, split = '[,.]')[[1]][1])
```

```{r}
# Create a family size variable including the passenger themselves
full$Fsize <- full$SibSp + full$Parch + 1

# Create a family variable 
full$Family <- paste(full$Surname, full$Fsize, sep='_')

# Discretize family size
full$FsizeD[full$Fsize == 1] <- 'single'
full$FsizeD[full$Fsize < 5 & full$Fsize > 1] <- 'small'
full$FsizeD[full$Fsize > 4] <- 'large'

```



```{r}
# Make variables factors into factors
factor_vars <- c('PassengerId','Pclass','Sex','Embarked',
                 'Title','Surname','Family','FsizeD')

full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))

# Set a random seed
set.seed(129)

# Perform mice imputation, excluding certain less-than-useful variables:
mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')], method='rf')

# Save the complete output 
mice_output <- complete(mice_mod)

# Replace Age variable from the mice model.
full$Age <- mice_output$Age

```

```{r}
# Split the dataset for modelling
train <- full[1:891,]
test <- full[892:1309,]

# Replace missing fare value with median for the passenger class 3 and those who embarked from Southampton
full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)

# Set as data.tables for one hot encoding / spare matrix
df <-data.table(train)
df2 <-data.table(test)
df2$Survived <- 0 # so NA values do not appear yet we can still proceed with one hot encoding, this will be dropped anyway

#create discreet groups for Age
head(df[, AgeDiscreet := as.factor(round(Age/10,0))])
head(df2[, AgeDiscreet := as.factor(round(Age/10,0))])

#Perfom one hot encoding to convert categorical in to dummy vars, removing our label (predicted) column
sparse_matrix <- sparse.model.matrix(Survived~.-1, data=df)

#Perfom one hot encoding to convert categorical in to dummy vars for our test data
sparse_matrix_test <- sparse.model.matrix(Survived~.-1, data=df2)
                                          
#could try grid search with caret see https://www.kaggle.com/jimthompson/ensemble-model-stacked-model-example and https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees

#Run cross validated xgboost
cv <- xgb.cv(data = sparse_matrix,
               label = df$Survived,
               max.depth = 6,
               eta = 0.3,
               nrounds = 1000,
               nfold = 10,
               objective = "reg:logistic",
               verbose = 2
)

# Get the evaluation log 
elog <- as.data.frame(cv$evaluation_log)

# Determine and print how many trees minimize training and test error
elog %>% 
   summarize(ntrees.train = which.min(elog$train_rmse_mean),   # find the index of min(train_rmse_mean)
             ntrees.test  = which.min(elog$test_rmse_mean))   # find the index of min(test_rmse_mean)

# Set the number of trees to use, as determined by xgb.cv 
ntrees  = which.min(elog$test_rmse_mean)

# Run xgb
xgb <- xgboost(data = sparse_matrix,
                      label = df$Survived,
                      max.depth = 6,
                      eta = 0.2,
                      nrounds = ntrees,
                      objective = "reg:logistic",
                      verbose = 2)

# Save the variables and print the top 10 most important factors
importance <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = xgb)
xgb.plot.importance(importance[1:10,])
xgb.plot.tree(feature_names = sparse_matrix@Dimnames[[2]], model = xgb)

# Make predictions on our original training data 
train$pred <- predict(xgb, sparse_matrix)

# Calculate error by convert the probability (as a decimal) to a binary outcome (1 or 0) with a 50% (0.5) cutoff point
err <- mean(as.numeric(train$pred > 0.5) != df$Survived)
print(paste("test-error=", err)) # calculate the amount of error

# Predict the data using the test data and convert to a binary
test$pred <- predict(xgb, sparse_matrix_test)

test$pred <- ifelse(test$pred > 0.5, 1, 0)

# Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
solution <- data.frame(PassengerID = test$PassengerId, Survived = test$pred)

# write solution
write.csv(solution, file = 'xgbSolution.csv', row.names = F)

# We can the explore the second and third order interactions outside of R using xgbi - code below creates the two files that are needed
# see http://projects.rajivshah.com/blog/2016/08/01/xgbfi/ 

featureList <- names(df[,-2]) # remove Survived variable
featureVector <- c() 
for (i in 1:length(featureList)) { 
  featureVector[i] <- paste(i-1, featureList[i], "q", sep="\t") 
}
write.table(featureVector, "fmap.txt", row.names=FALSE, quote = FALSE, col.names = FALSE)
xgb.dump(model = xgb, fname = 'xgb.dump', with_stats = TRUE)

featureList <- sparse_matrix@Dimnames[[2]]  # this is the correct variable names, but xgb.dump does not appear to like them - most likely the name variable
featureVector <- c() 
for (i in 1:length(featureList)) { 
  featureVector[i] <- paste(i-1, featureList[i], "q", sep="-") 
}
write.table(featureVector, "fmap.txt", row.names=FALSE, quote = FALSE, col.names = FALSE)

```

